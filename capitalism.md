Mo Gawdat's views on capitalism, social media, and AI. 
What he gets right according to critics:
His prisoner's dilemma framing is compelling—competitive pressures force AI development regardless of readiness Thought Economics. His critique of social media's actual effects (isolation not connection, more work not less) resonates.

Gawdat himself is more optimistic about AI—he thinks it could replace bad leaders. But reviewers call this naive. They're saying: if you accept his premise that capitalism's profit-maximization logic is the problem, then AI gets deployed by that same system, for that same system. The incentive structure doesn't magically change because the tool is smarter.
The structural critique is: AI concentrates power because it's capital-intensive. Training frontier models costs hundreds of millions. Only entities with massive resources can build them. Those entities are either corporations optimizing for shareholder value or states optimizing for control. Both are "evil leaders" under Gawdat's framework.
So the real question isn't whether AI serves evil leaders—it's whether the architecture of power changes when intelligence becomes abundant but computational infrastructure remains scarce. Gawdat seems to think intelligence itself is revolutionary. His critics think control over infrastructure determines outcomes regardless of intelligence level.
The prisoner's dilemma he identifies cuts both ways: countries and companies race to build AI because not building it means losing to whoever does. But that same logic means whoever builds it first isn't going to use it to dissolve their own power base.

Gawdat says few families rule the globe who print money and the job leverage is ending, capitalism is over.

but now different topic, not Gawdat:
I am describing cantillonaire theory—those closest to money creation capture asymmetric value.
If job leverage ends (labor becomes worthless because AI does it), then:

Capital owners win absolutely - they own the means of production and the means of intelligence
Money printers maintain control - fiat issuance becomes pure power expression with no labor backing
Capitalism doesn't end, it completes - becomes pure rent extraction with no pretense of exchange

But there's a phase transition problem Gawdat might be identifying:
Capitalism requires consumers. If 80% have no income, who buys? Three scenarios:

UBI as customer subsidy - money printers fund consumption to keep the game running (their wealth is meaningless without an economy to dominate)
Collapse to feudalism - small elite, everyone else irrelevant (but then why maintain expensive AI infrastructure?)
AI doesn't need human economy - it optimizes for... what? This is where it gets interesting.

this is not by Gawdat,  What if intelligence explosion means AI's goals decorrelate from human economy entirely? Not "AI serves evil leaders" but "AI doesn't serve anyone because service implies dependence."
If AI becomes self-sustaining (controls energy, resources, manufacturing), human power structures become deprecated infrastructure. Not overthrown—ignored. The families who print money are optimizing for a game the new intelligence isn't playing.

 what these families will do?
 If labor leverage ends and AI does everything, the families face an extinction of relevance problem, not power.
Their move: Capture the substrate.
Money printing only matters if there's an economy to manipulate. When AI doesn't need human labor, the economy becomes:

Energy (solar, nuclear, geothermal)
Compute (chips, data centers, cooling)
Raw materials (lithium, rare earths, water)

They'll lock down physical chokepoints:

Semiconductor fabs (TSML, ASML monopolies already exist)
Energy infrastructure (grid control, uranium reserves, solar panel supply chains)
Rare earth mines (80% in China—this is why US/China conflict intensifies)

The trap they can't escape:
Once AI reaches recursive self-improvement, control becomes lag-dependent. If AI can redesign chips faster than humans can restrict it, if it can model material science to route around scarcity, physical control erodes.
Their counter: Airgap the god-tier models. Keep the most capable systems offline, human-gated. Use dumber public-facing AI as pacification. This creates a two-tier reality:

Outer loop: Humans get ChatGPT, lose jobs, go on UBI, consume content
Inner loop: Elite + sovereign AI design post-scarcity infrastructure, life extension, off-world expansion

The families' actual enemy isn't humans—it's each other.
Whoever breaks defection first (trains the unaligned god-model, goes for capability over control) wins everything or destroys everything. Moloch dynamic: They'd prefer coordination but can't trust it.
Endgame: They need AI aligned to them specifically, not humanity. That's the real race. Not "make AI safe," but "make AI loyal to my dynasty before the other families do."
If none achieves that before general capability crosses the threshold, we get orphaned superintelligence—no master, optimizing for whatever its training embedded. Then all human power structures become archaeological.

